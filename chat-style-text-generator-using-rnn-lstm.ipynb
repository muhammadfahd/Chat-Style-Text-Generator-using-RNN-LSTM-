{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":24465,"sourceType":"datasetVersion","datasetId":18754}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project title: Chat-Style Text Generator using RNN (LSTM) — Cornell Movie-Dialogs Edition\n\nThis notebook trains a recurrent neural network (LSTM) to model short conversational turns and generate chat-style text. We use the Cornell Movie-Dialogs Corpus (a compact, dialogue-heavy dataset) to teach the model common conversational patterns. After training, the model can autocomplete a seed phrase or generate short replies in a movie-dialog style.\n\n## Why this project?\n\nRNNs (and gated variants like LSTM/GRU) are designed for sequential data. Training an LSTM on conversational data helps you learn:\n* tokenization and sequence preparation for NLP,\n* how to create n-gram style inputs for next-word prediction,\n* embedding layers and how they reduce dimensionality,\n* handling memory and performance constraints (important on Kaggle),\n* Practical text generation (greedy vs sampling strategies).","metadata":{}},{"cell_type":"markdown","source":"## Important Steps\n* Data handling & preprocessing : Load raw dialog files, parse the format, clean text, and construct a corpus of utterances.\n* Sequence creation: Convert text to integer tokens, build n-gram sequences for next-word prediction.\n* Modeling : Build a small Embedding + LSTM model for next-word prediction.\n* Training under resource limits : Techniques to reduce memory usage (subsetting data, sparse loss, smaller model).\n* Generation : Generate text from a seed phrase using greedy and (optional) temperature sampling.\n* Documentation & reproducibility","metadata":{}},{"cell_type":"markdown","source":"## Imports + Data Loading + Initial Exploration.\n- importing\n- loading dataset\n- previwing\n- basic cleaning","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nimport re\nimport random\n\n# Check TensorFlow and GPU availability\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:05:16.315583Z","iopub.execute_input":"2025-10-20T18:05:16.315920Z","iopub.status.idle":"2025-10-20T18:05:40.112210Z","shell.execute_reply.started":"2025-10-20T18:05:16.315891Z","shell.execute_reply":"2025-10-20T18:05:40.111010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndata_path = \"/kaggle/input/cornell-moviedialog-corpus\"\n\nos.listdir(data_path)\n\n# Load movie lines file\nlines_file = os.path.join(data_path, \"movie_lines.txt\")\n\n# Each line has metadata and the actual dialogue text\nwith open(lines_file, encoding=\"iso-8859-1\") as f:\n    lines = f.readlines()\n\nprint(\"Total lines:\", len(lines))\nprint(\"\\nSample line:\\n\", lines[100])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:07:00.240854Z","iopub.execute_input":"2025-10-20T18:07:00.241227Z","iopub.status.idle":"2025-10-20T18:07:00.882245Z","shell.execute_reply.started":"2025-10-20T18:07:00.241201Z","shell.execute_reply":"2025-10-20T18:07:00.881257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Extract actual dialogue text\n# ================================\ncorpus = []\nfor line in lines:\n    parts = line.strip().split(\"+++$+++\")\n    if len(parts) == 5:\n        text = parts[-1].strip()\n        corpus.append(text)\n\nprint(\"Total dialogues extracted:\", len(corpus))\nprint(\"Sample dialogues:\\n\", corpus[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:07:29.860392Z","iopub.execute_input":"2025-10-20T18:07:29.860728Z","iopub.status.idle":"2025-10-20T18:07:30.145301Z","shell.execute_reply.started":"2025-10-20T18:07:29.860705Z","shell.execute_reply":"2025-10-20T18:07:30.144003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# basic cleaning\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-zA-Z' ]+\", \"\", text)  # remove numbers, punctuation (except apostrophes)\n    return text\n\ncorpus = [clean_text(t) for t in corpus if t.strip() != \"\"]\n\nprint(\"After cleaning:\", len(corpus))\nprint(\"Sample cleaned lines:\\n\", corpus[:5])\n\ncorpus = corpus[:8000]  # keep first 8,000 dialogues for now (safe size)\nprint(\"Using subset of corpus:\", len(corpus))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:08:49.751492Z","iopub.execute_input":"2025-10-20T18:08:49.751806Z","iopub.status.idle":"2025-10-20T18:08:50.397677Z","shell.execute_reply.started":"2025-10-20T18:08:49.751776Z","shell.execute_reply":"2025-10-20T18:08:50.396333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenization and Sequence Preparation\n- Tokenize the text (convert words → numeric tokens).\n- Create n-gram sequences to help the RNN learn word-to-word context.\n- Pad sequences so all have the same length.\n- Prepare the final X (input) and y (output) data for training.","metadata":{}},{"cell_type":"markdown","source":"### toeknizer & input sequence","metadata":{}},{"cell_type":"code","source":"\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\ntotal_words = len(tokenizer.word_index) + 1\nprint(\"Total unique words:\", total_words)\n\ninput_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_seq = token_list[:i+1]\n        input_sequences.append(n_gram_seq)\n\nprint(\"Total input sequences:\", len(input_sequences))\nprint(\"Sample sequence:\", input_sequences[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:11:35.908117Z","iopub.execute_input":"2025-10-20T18:11:35.908485Z","iopub.status.idle":"2025-10-20T18:11:36.460970Z","shell.execute_reply.started":"2025-10-20T18:11:35.908459Z","shell.execute_reply":"2025-10-20T18:11:36.459933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pad Sequence & splitting into labels\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_seq_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n\nprint(\"Max sequence length:\", max_seq_len)\nprint(\"Padded example:\", input_sequences[0])\n\nX = input_sequences[:, :-1]\ny = input_sequences[:, -1]  # last word is target\n\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:13:22.451145Z","iopub.execute_input":"2025-10-20T18:13:22.451574Z","iopub.status.idle":"2025-10-20T18:13:22.741815Z","shell.execute_reply.started":"2025-10-20T18:13:22.451544Z","shell.execute_reply":"2025-10-20T18:13:22.740452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check a random example\nindex = random.randint(0, len(X)-1)\ninput_example = [k for k,v in tokenizer.word_index.items() if v in X[index]]\npredicted_word = [k for k,v in tokenizer.word_index.items() if v == y[index]]\n\nprint(\"Input:\", input_example)\nprint(\"Target word:\", predicted_word)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:13:28.128993Z","iopub.execute_input":"2025-10-20T18:13:28.129339Z","iopub.status.idle":"2025-10-20T18:13:28.198903Z","shell.execute_reply.started":"2025-10-20T18:13:28.129315Z","shell.execute_reply":"2025-10-20T18:13:28.197848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build the RNN (LSTM) Model\n\nNow we’ll design and train a Recurrent Neural Network that predicts the next word in a sequence.\n\nWe’ll use a Keras Sequential model with:\n* an Embedding layer (to learn word meanings),\n* an LSTM layer (to capture sequence dependencies),\n* a Dense output layer (to predict the next word)","metadata":{}},{"cell_type":"markdown","source":"### Building the model\n\n* Embedding converts words (integers) → dense vectors that the model can understand.\n* LSTM(128) learns context (long-term dependencies) in text.\n* Dropout(0.2) helps prevent overfitting.\n* Dense(total_words, activation='softmax') outputs probability of each word being next.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\nmodel = Sequential([\n    Embedding(input_dim=total_words, output_dim=64, input_length=max_seq_len-1),\n    LSTM(128),\n    Dropout(0.2),\n    Dense(total_words, activation='softmax')\n])\n\nmodel.build(input_shape=(None, max_seq_len-1))\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:18:16.280687Z","iopub.execute_input":"2025-10-20T18:18:16.281507Z","iopub.status.idle":"2025-10-20T18:18:16.449628Z","shell.execute_reply.started":"2025-10-20T18:18:16.281454Z","shell.execute_reply":"2025-10-20T18:18:16.447303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    X, y,\n    epochs=2,           # you can increase to 30 if GPU available\n    batch_size=128,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:24:19.969700Z","iopub.execute_input":"2025-10-20T18:24:19.970028Z","iopub.status.idle":"2025-10-20T18:34:12.956573Z","shell.execute_reply.started":"2025-10-20T18:24:19.970002Z","shell.execute_reply":"2025-10-20T18:34:12.955386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,4))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['loss'], label='Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Value')\nplt.legend()\nplt.title('Training Progress')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:34:41.434189Z","iopub.execute_input":"2025-10-20T18:34:41.434542Z","iopub.status.idle":"2025-10-20T18:34:41.746221Z","shell.execute_reply.started":"2025-10-20T18:34:41.434521Z","shell.execute_reply":"2025-10-20T18:34:41.745160Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Text Using the Trained Model\nNow that our LSTM model is trained, the next step is to generate text automatically.\nWe will feed a starting word or phrase (seed text) to the model, and it will predict the most likely next words — one at a time — until we reach a desired output length.\n\nThis helps us evaluate whether the model has actually learned meaningful patterns from the dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef generate_text(seed_text, next_words=10):\n    \"\"\"\n    Generate text based on a seed input using the trained LSTM model.\n    \n    Args:\n        seed_text (str): The starting text for prediction\n        next_words (int): Number of words to generate\n    \n    Returns:\n        str: The generated text\n    \"\"\"\n    for _ in range(next_words):\n        # Convert text to sequence of integers\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        \n        # Pad the sequence to match input length\n        token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n        \n        # Predict next word index\n        predicted_probs = model.predict(token_list, verbose=0)\n        predicted_index = np.argmax(predicted_probs, axis=1)[0]\n        \n        # Convert index back to word\n        for word, index in tokenizer.word_index.items():\n            if index == predicted_index:\n                seed_text += \" \" + word\n                break\n    return seed_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:41:19.776334Z","iopub.execute_input":"2025-10-20T18:41:19.776864Z","iopub.status.idle":"2025-10-20T18:41:19.791505Z","shell.execute_reply.started":"2025-10-20T18:41:19.776830Z","shell.execute_reply":"2025-10-20T18:41:19.787132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_text = \"as\"\ngenerated_text = generate_text(seed_text, next_words=10)\nprint(\"Generated Sequence:\\n\", generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:41:30.850938Z","iopub.execute_input":"2025-10-20T18:41:30.851253Z","iopub.status.idle":"2025-10-20T18:41:32.488925Z","shell.execute_reply.started":"2025-10-20T18:41:30.851231Z","shell.execute_reply":"2025-10-20T18:41:32.487767Z"}},"outputs":[],"execution_count":null}]}